Lexing
======

Definition
----------

> In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens 

Tokens
------

Commonly stored as (name, value) tuples. Example:

```python
my_variable = 17
```

Becomes:

```
(identifier, "my_variable")
(operator, "=")
(integer, 17)
```

Calculator Example
------------------

Supported operations
--------------------

- `1 + 2`
- `4 - 2`
- `2 * 3`
- `8 / 4`

Goal
----

- Scan a string such as "1 + 2 * 3" and create a list of (type, value) tuples:

```python
(number, 1)
(operator, +)
(number, 2)
(operator, *)
(number, 3)
```

Chomsky Hierarchy
-----------------

Grammar       Automaton (Computer)
------------- ----------------------
Unrestricted  Turing Machines
Context Free  Pushdown Automata
Regular       Finite State Automata

Regular Languages
-----------------

- Are insufficient to parse most programming languages
- Are useful for parsing tokens
- Can be processed by a simple finite automaton

Deterministic Finite Automaton (DFA)
------------------------------------

- Finite set of states $Q$
- Finite set of input symbols called the alphabet $\Sigma$
- Transition function $\delta : Q \times \Sigma \rightarrow Q$
- Initial or start state $q_0 \in Q$
- Set of accept states $F \subseteq Q$

Drawing DFAs
------------

- States are nodes on the graph
- Start state indicated by arrow
- Accept states indicated by double border
- Transitions indicated as labelled arrows

---

![DFA to accept string containing an even number of zeroes](https://upload.wikimedia.org/wikipedia/commons/9/9d/DFAexample.svg)

Regular Expressions
-------------------

A string used as a search pattern for a member of a regular language.

RE Basics
---------

- Characters are generally matched literally.

```python
import re

s = "the quick brown fox jumped over the lazy dog"
re.findall("dog", s)
```

```python
['dog']
```

RE Boolean Or
-------------

- Pipes (|) can be used for boolean or

```python
import re

s = "the quick brown fox jumped over the lazy dog"
re.findall("dog|fox", s)
```

```python
['fox', 'dog']
```

RE Quantifiers
--------------

- `?` - zero or one occurences of preceding element
- `*` - zero or more occurences of preceding element
- `+` - one or more occurences of preceding element
- `{n}` - exactly n occurences of preceding element

---

```python
import re

s = "the quick brown fox jumped over the lazy dog"
re.findall("ov?", s)
```

```python
['o', 'o', 'ov', 'o']
```

RE Grouping
-----------

- Parens can be used for grouping

```python
import re

s = "the quick brown fox jumped over the lazy dog"
re.findall("(d|f)o(g|x)", s)
```

```python
[('fox', 'f', 'x'), ('dog', 'd', 'g')]
```

RE Bracket Expressions
----------------------

- Brackets `[]` may be used to match a single character against a set of characters

```python
import re

s = "the quick brown fox jumped over the lazy dog"
re.findall("[df]o[gx]", s)
```

```python
['fox', 'dog']
```

RE Character Classes
--------------------

Several special character classes are provided:

- `\w` - alphanumeric characters
- `\d` - digits
- `\s` - whitespace characters
- `.` - anything

---

```python
import re

s = "the quick brown fox jumped over the lazy dog"
re.findall("\s...\s", s)
```

```python
[' fox ', ' the ']
```

DFA RE Equivalence
------------------

- A DFA can be created to match any regular expression
- A regular expression can be created to match a language accepted by any DFA

Simple Lexer in Python
----------------------

```python
import sys
import re

def lex_one(value):
    if re.match('\d+', value):
        return ('number', value)
    elif re.match('[\+\-\*\/]', value):
        return ('operator', value)
    else:
        raise ValueError(f'Invalid token: {value}')

def lex(text):
    return [lex_one(v) for v in text.split()]
```
