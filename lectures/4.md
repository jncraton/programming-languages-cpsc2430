Lexing
======

Definition
----------

> In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens 

Tokens
------

Commonly stored as (name, value) tuples. Example:

```python
my_variable = 17
```

Becomes:

```
(identifier, "my_variable")
(operator, "=")
(integer, 17)
```

Calculator Example
------------------

Supported operations
--------------------

- `1 + 2`
- `4 - 2`
- `2 * 3`
- `8 / 4`

Goal
----

- Scan a string such as "1 + 2 * 3" and create a list of (type, value) tuples:

```python
(number, 1)
(operator, +)
(number, 2)
(operator, *)
(number, 3)
```

Simple Lexer in Python
----------------------

```python
import sys
import re

def lex_one(value):
    if re.match('\d+', value):
        return ('number', value)
    elif re.match('[\+\-\*\/]', value):
        return ('operator', value)
    else:
        raise ValueError(f'Invalid token: {value}')

def lex(text):
    return [lex_one(v) for v in text.split()]

print(lex(sys.argv[1]))
```
